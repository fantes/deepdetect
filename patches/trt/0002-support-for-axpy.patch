diff --git a/CMakeLists.txt b/CMakeLists.txt
index 4b5e148..41e5ca2 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -137,11 +137,7 @@ if (DEFINED GPU_ARCHS)
   separate_arguments(GPU_ARCHS)
 else()
   list(APPEND GPU_ARCHS
-      35
-      53
       61
-      70
-      75
     )
 
   string(REGEX MATCH "aarch64" IS_ARM "${TRT_PLATFORM_ID}")
diff --git a/parsers/caffe/CMakeLists.txt b/parsers/caffe/CMakeLists.txt
index 285da1b..10dffbf 100644
--- a/parsers/caffe/CMakeLists.txt
+++ b/parsers/caffe/CMakeLists.txt
@@ -73,7 +73,7 @@ target_compile_definitions(${SHARED_TARGET}
     "-DGOOGLE_PROTOBUF_ARCH_64_BIT"
 )
 
-set_target_properties(${SHARED_TARGET} PROPERTIES LINK_FLAGS "-Wl,--exclude-libs,ALL")
+#set_target_properties(${SHARED_TARGET} PROPERTIES LINK_FLAGS "-Wl,--exclude-libs,ALL")
 
 set_target_properties(${SHARED_TARGET} PROPERTIES DEBUG_POSTFIX ${TRT_DEBUG_POSTFIX})
 
@@ -91,13 +91,13 @@ add_dependencies(${STATIC_TARGET} caffe_proto)
 
 target_include_directories(${STATIC_TARGET}
     PUBLIC ${PROJECT_SOURCE_DIR}/include
-    PRIVATE .
-    PRIVATE caffeParser
-    PRIVATE caffeParser/opParsers
-    PRIVATE caffeWeightFactory
-    PRIVATE ../common
-    PRIVATE ${Protobuf_INCLUDE_DIR}
-    PRIVATE ${CMAKE_CURRENT_BINARY_DIR}/proto
+    PUBLIC .
+    PUBLIC caffeParser
+    PUBLIC caffeParser/opParsers
+    PUBLIC caffeWeightFactory
+    PUBLIC ../common
+    PUBLIC ${Protobuf_INCLUDE_DIR}
+    PUBLIC ${CMAKE_CURRENT_BINARY_DIR}/proto
 )
 
 set_target_properties(${STATIC_TARGET} 
@@ -117,12 +117,12 @@ target_link_libraries(${STATIC_TARGET}
 # modify google namespace to avoid namespace collision.
 set(GOOGLE google_private)
 target_compile_definitions(${STATIC_TARGET}
-    PRIVATE 
+    PUBLIC
     "-Dgoogle=${GOOGLE}"
     "-DGOOGLE_PROTOBUF_ARCH_64_BIT"
 )
 
-set_target_properties(${STATIC_TARGET} PROPERTIES LINK_FLAGS "-Wl,--exclude-libs,ALL")
+#set_target_properties(${STATIC_TARGET} PROPERTIES LINK_FLAGS "-Wl,--exclude-libs,ALL")
 
 set_target_properties(${STATIC_TARGET} PROPERTIES DEBUG_POSTFIX ${TRT_DEBUG_POSTFIX})
 
diff --git a/parsers/caffe/caffeParser/caffeParser.cpp b/parsers/caffe/caffeParser/caffeParser.cpp
index 9fb28d6..baa27c2 100644
--- a/parsers/caffe/caffeParser/caffeParser.cpp
+++ b/parsers/caffe/caffeParser/caffeParser.cpp
@@ -361,9 +361,7 @@ const IBlobNameToTensor* CaffeParser::parse(const char* deployFile,
     return parse(network, weightType, modelFile != nullptr);
 }
 
-const IBlobNameToTensor* CaffeParser::parse(INetworkDefinition& network,
-                                            DataType weightType,
-                                            bool hasModel)
+const IBlobNameToTensor* CaffeParser::parse(INetworkDefinition& network, DataType weightType, bool hasModel)
 {
     bool ok = true;
     CaffeWeightFactory weights(*mModel.get(), weightType, mTmpAllocs, hasModel);
@@ -391,25 +389,31 @@ const IBlobNameToTensor* CaffeParser::parse(INetworkDefinition& network,
         {
             if (mDeploy->input_shape_size())
             {
-                dims = DimsCHW{(int) mDeploy->input_shape().Get(i).dim().Get(1), (int) mDeploy->input_shape().Get(i).dim().Get(2), (int) mDeploy->input_shape().Get(i).dim().Get(3)};
+                dims = DimsCHW{(int) mDeploy->input_shape().Get(i).dim().Get(1),
+                    (int) mDeploy->input_shape().Get(i).dim().Get(2), (int) mDeploy->input_shape().Get(i).dim().Get(3)};
             }
             else
             {
                 // Deprecated, but still used in a lot of networks
-                dims = DimsCHW{(int) mDeploy->input_dim().Get(i * 4 + 1), (int) mDeploy->input_dim().Get(i * 4 + 2), (int) mDeploy->input_dim().Get(i * 4 + 3)};
+                dims = DimsCHW{(int) mDeploy->input_dim().Get(i * 4 + 1), (int) mDeploy->input_dim().Get(i * 4 + 2),
+                    (int) mDeploy->input_dim().Get(i * 4 + 3)};
             }
         }
         else
         {
-            std::cout << "Warning, setting batch size to 1. Update the dimension after parsing due to using explicit batch size." << std::endl;
+            std::cout << "Warning, setting batch size to 1. Update the dimension after parsing due to using explicit "
+                         "batch size."
+                      << std::endl;
             if (mDeploy->input_shape_size())
             {
-                dims = DimsNCHW{1, (int) mDeploy->input_shape().Get(i).dim().Get(1), (int) mDeploy->input_shape().Get(i).dim().Get(2), (int) mDeploy->input_shape().Get(i).dim().Get(3)};
+                dims = DimsNCHW{1, (int) mDeploy->input_shape().Get(i).dim().Get(1),
+                    (int) mDeploy->input_shape().Get(i).dim().Get(2), (int) mDeploy->input_shape().Get(i).dim().Get(3)};
             }
             else
             {
                 // Deprecated, but still used in a lot of networks
-                dims = DimsNCHW{1, (int) mDeploy->input_dim().Get(i * 4 + 1), (int) mDeploy->input_dim().Get(i * 4 + 2), (int) mDeploy->input_dim().Get(i * 4 + 3)};
+                dims = DimsNCHW{1, (int) mDeploy->input_dim().Get(i * 4 + 1), (int) mDeploy->input_dim().Get(i * 4 + 2),
+                    (int) mDeploy->input_dim().Get(i * 4 + 3)};
             }
         }
         ITensor* tensor = network.addInput(mDeploy->input().Get(i).c_str(), DataType::kFLOAT, dims);
@@ -436,7 +440,9 @@ const IBlobNameToTensor* CaffeParser::parse(INetworkDefinition& network,
                     if (iter != nullptr && iter->isNetworkInput())
                     {
                         ok = false;
-                        std::cout << "TensorRT does not support in-place operations on input tensors in a prototxt file." << std::endl;
+                        std::cout
+                            << "TensorRT does not support in-place operations on input tensors in a prototxt file."
+                            << std::endl;
                     }
                 }
             }
@@ -446,17 +452,20 @@ const IBlobNameToTensor* CaffeParser::parse(INetworkDefinition& network,
         if (mPluginFactory && mPluginFactory->isPlugin(layerMsg.name().c_str()))
         {
             std::vector<Weights> w = weights.getAllWeights(layerMsg.name());
-            IPlugin* plugin = mPluginFactory->createPlugin(layerMsg.name().c_str(), w.empty() ? nullptr : &w[0], w.size());
+            IPlugin* plugin
+                = mPluginFactory->createPlugin(layerMsg.name().c_str(), w.empty() ? nullptr : &w[0], w.size());
             std::vector<ITensor*> inputs;
             for (int i = 0, n = layerMsg.bottom_size(); i < n; i++)
             {
                 inputs.push_back((*mBlobNameToTensor)[layerMsg.bottom(i)]);
             }
 
-            bool isExt = mPluginFactoryIsExt && static_cast<IPluginFactoryExt*>(mPluginFactory)->isPluginExt(layerMsg.name().c_str());
+            bool isExt = mPluginFactoryIsExt
+                && static_cast<IPluginFactoryExt*>(mPluginFactory)->isPluginExt(layerMsg.name().c_str());
 
-            ILayer* layer = isExt ? network.addPluginExt(&inputs[0], int(inputs.size()), *static_cast<IPluginExt*>(plugin))
-                                  : network.addPlugin(&inputs[0], int(inputs.size()), *plugin);
+            ILayer* layer = isExt
+                ? network.addPluginExt(&inputs[0], int(inputs.size()), *static_cast<IPluginExt*>(plugin))
+                : network.addPlugin(&inputs[0], int(inputs.size()), *plugin);
 
             layer->setName(layerMsg.name().c_str());
             if (plugin->getNbOutputs() != layerMsg.top_size())
@@ -483,10 +492,13 @@ const IBlobNameToTensor* CaffeParser::parse(INetworkDefinition& network,
             {
                 if (mPluginFactory)
                 {
-                    RETURN_AND_LOG_ERROR(nullptr, "Both IPluginFactory and IPluginFactoryV2 are set. If using TensorRT 5.0 or later, switch to IPluginFactoryV2");
+                    RETURN_AND_LOG_ERROR(nullptr,
+                        "Both IPluginFactory and IPluginFactoryV2 are set. If using TensorRT 5.0 or later, switch to "
+                        "IPluginFactoryV2");
                 }
                 std::vector<Weights> w = weights.getAllWeights(layerMsg.name());
-                nvinfer1::IPluginV2* plugin = mPluginFactoryV2->createPlugin(layerMsg.name().c_str(), w.empty() ? nullptr : &w[0], w.size(), mPluginNamespace.c_str());
+                nvinfer1::IPluginV2* plugin = mPluginFactoryV2->createPlugin(
+                    layerMsg.name().c_str(), w.empty() ? nullptr : &w[0], w.size(), mPluginNamespace.c_str());
                 std::vector<ITensor*> inputs;
                 for (int i = 0, n = layerMsg.bottom_size(); i < n; i++)
                 {
@@ -513,66 +525,70 @@ const IBlobNameToTensor* CaffeParser::parse(INetworkDefinition& network,
             }
             // Use the TRT5 plugin creator method to check for built-in plugin support
 
+            std::string pluginName;
+            nvinfer1::PluginFieldCollection fc;
+            std::vector<nvinfer1::PluginField> f;
+            if (layerMsg.type() == "Normalize")
+            {
+                pluginName = "Normalize_TRT";
+                f = parseNormalizeParam(layerMsg, weights, *mBlobNameToTensor);
+            }
+            else if (layerMsg.type() == "PriorBox")
+            {
+                pluginName = "PriorBox_TRT";
+                f = parsePriorBoxParam(layerMsg, weights, *mBlobNameToTensor);
+            }
+            else if (layerMsg.type() == "DetectionOutput")
+            {
+                pluginName = "NMS_TRT";
+                f = parseDetectionOutputParam(layerMsg, weights, *mBlobNameToTensor);
+            }
+            else if (layerMsg.type() == "RPROI")
+            {
+                pluginName = "RPROI_TRT";
+                f = parseRPROIParam(layerMsg, weights, *mBlobNameToTensor);
+            }
+            else if (layerMsg.type() == "Axpy")
+            {
+                pluginName = "AXPY_TRT";
+                // no params, no weights
+            }
 
-                std::string pluginName;
-                nvinfer1::PluginFieldCollection fc;
-                std::vector<nvinfer1::PluginField> f;
-                if (layerMsg.type() == "Normalize")
-                {
-                    pluginName = "Normalize_TRT";
-                    f = parseNormalizeParam(layerMsg, weights, *mBlobNameToTensor);
-                }
-                else if (layerMsg.type() == "PriorBox")
+            if (mPluginRegistry.find(pluginName) != mPluginRegistry.end())
+            {
+                // Set fc
+                fc.nbFields = f.size();
+                fc.fields = f.empty() ? nullptr : f.data();
+                nvinfer1::IPluginV2* pluginV2
+                    = mPluginRegistry.at(pluginName)->createPlugin(layerMsg.name().c_str(), &fc);
+                assert(pluginV2);
+                mNewPlugins.push_back(pluginV2);
+
+                std::vector<ITensor*> inputs;
+                for (int i = 0, n = layerMsg.bottom_size(); i < n; i++)
                 {
-                    pluginName = "PriorBox_TRT";
-                    f = parsePriorBoxParam(layerMsg, weights, *mBlobNameToTensor);
+                    inputs.push_back((*mBlobNameToTensor)[layerMsg.bottom(i)]);
                 }
-                else if (layerMsg.type() == "DetectionOutput")
+
+                auto layer = network.addPluginV2(&inputs[0], int(inputs.size()), *pluginV2);
+                layer->setName(layerMsg.name().c_str());
+                if (pluginV2->getNbOutputs() != layerMsg.top_size())
                 {
-                    pluginName = "NMS_TRT";
-                    f = parseDetectionOutputParam(layerMsg, weights, *mBlobNameToTensor);
+                    std::cout << "Plugin layer output count is not equal to caffe output count" << std::endl;
+                    ok = false;
                 }
-                else if (layerMsg.type() == "RPROI")
+                for (int i = 0, n = std::min(layer->getNbOutputs(), layerMsg.top_size()); i < n; i++)
                 {
-                    pluginName = "RPROI_TRT";
-                    f = parseRPROIParam(layerMsg, weights, *mBlobNameToTensor);
+                    (*mBlobNameToTensor)[layerMsg.top(i)] = layer->getOutput(i);
                 }
 
-                if (mPluginRegistry.find(pluginName) != mPluginRegistry.end())
+                if (layer == nullptr)
                 {
-                    // Set fc
-                    fc.nbFields = f.size();
-                    fc.fields = f.empty() ? nullptr : f.data();
-                    nvinfer1::IPluginV2* pluginV2 = mPluginRegistry.at(pluginName)->createPlugin(layerMsg.name().c_str(), &fc);
-                    assert(pluginV2);
-                    mNewPlugins.push_back(pluginV2);
-
-                    std::vector<ITensor*> inputs;
-                    for (int i = 0, n = layerMsg.bottom_size(); i < n; i++)
-                    {
-                        inputs.push_back((*mBlobNameToTensor)[layerMsg.bottom(i)]);
-                    }
-
-                    auto layer = network.addPluginV2(&inputs[0], int(inputs.size()), *pluginV2);
-                    layer->setName(layerMsg.name().c_str());
-                    if (pluginV2->getNbOutputs() != layerMsg.top_size())
-                    {
-                        std::cout << "Plugin layer output count is not equal to caffe output count" << std::endl;
-                        ok = false;
-                    }
-                    for (int i = 0, n = std::min(layer->getNbOutputs(), layerMsg.top_size()); i < n; i++)
-                    {
-                        (*mBlobNameToTensor)[layerMsg.top(i)] = layer->getOutput(i);
-                    }
-
-                    if (layer == nullptr)
-                    {
-                        std::cout << "error parsing layer type " << layerMsg.type() << " index " << i << std::endl;
-                        ok = false;
-                    }
-                    continue;
+                    std::cout << "error parsing layer type " << layerMsg.type() << " index " << i << std::endl;
+                    ok = false;
                 }
-
+                continue;
+            }
         }
 
         if (layerMsg.type() == "Dropout")
@@ -589,7 +605,8 @@ const IBlobNameToTensor* CaffeParser::parse(INetworkDefinition& network,
                 const trtcaffe::BlobShape& shape = p.shape().Get(i);
                 if (shape.dim_size() != 4)
                 {
-                    RETURN_AND_LOG_ERROR(nullptr, "error parsing input layer, TensorRT only supports 4 dimensional input");
+                    RETURN_AND_LOG_ERROR(
+                        nullptr, "error parsing input layer, TensorRT only supports 4 dimensional input");
                 }
                 else
                 {
@@ -632,7 +649,8 @@ const IBlobNameToTensor* CaffeParser::parse(INetworkDefinition& network,
         }
         else
         {
-            ILayer* layer = (*v->second)(network, layerMsg, weights, *static_cast<BlobNameToTensor*>(mBlobNameToTensor));
+            ILayer* layer
+                = (*v->second)(network, layerMsg, weights, *static_cast<BlobNameToTensor*>(mBlobNameToTensor));
             if (layer == nullptr)
             {
                 std::cout << "error parsing layer type " << layerMsg.type() << " index " << i << std::endl;
diff --git a/plugin/CMakeLists.txt b/plugin/CMakeLists.txt
index 940392a..7420c14 100644
--- a/plugin/CMakeLists.txt
+++ b/plugin/CMakeLists.txt
@@ -54,6 +54,7 @@ set(PLUGIN_LISTS
     groupNormalizationPlugin
     coordConvACPlugin
     leakyReluPlugin
+    axpyPlugin
     )
 
 # Add BERT sources if ${BERT_GENCODES} was populated
@@ -99,9 +100,9 @@ add_library(${SHARED_TARGET} SHARED
 target_include_directories(${SHARED_TARGET}
     PUBLIC ${PROJECT_SOURCE_DIR}/include
     PUBLIC ${CUB_ROOT_DIR}
-    PRIVATE ${PROJECT_SOURCE_DIR}/common
+    PUBLIC ${PROJECT_SOURCE_DIR}/common
     PUBLIC ${CUDA_INSTALL_DIR}/include
-    PRIVATE ${TARGET_DIR}
+    PUBLIC ${TARGET_DIR}
 )
 
 set_target_properties(${SHARED_TARGET} PROPERTIES
@@ -113,7 +114,7 @@ set_target_properties(${SHARED_TARGET} PROPERTIES
     RUNTIME_OUTPUT_DIRECTORY "${TRT_OUT_DIR}"
 )
 
-set_target_properties(${SHARED_TARGET} PROPERTIES LINK_FLAGS "-Wl,--exclude-libs,ALL -Wl,--version-script=${PLUGIN_EXPORT_MAP} -Wl,--no-undefined")
+set_target_properties(${SHARED_TARGET} PROPERTIES LINK_FLAGS "-Wl,--no-undefined")
 
 set_target_properties(${SHARED_TARGET} PROPERTIES DEBUG_POSTFIX ${TRT_DEBUG_POSTFIX})
 
@@ -139,9 +140,9 @@ add_library(${STATIC_TARGET} STATIC
 target_include_directories(${STATIC_TARGET}
     PUBLIC ${PROJECT_SOURCE_DIR}/include
     PUBLIC ${CUB_ROOT_DIR}
-    PRIVATE ${PROJECT_SOURCE_DIR}/common
+    PUBLIC ${PROJECT_SOURCE_DIR}/common
     PUBLIC ${CUDA_INSTALL_DIR}/include
-    PRIVATE ${TARGET_DIR}
+    PUBLIC ${TARGET_DIR}
 )
 
 set_target_properties(${STATIC_TARGET} PROPERTIES
@@ -153,7 +154,7 @@ set_target_properties(${STATIC_TARGET} PROPERTIES
     RUNTIME_OUTPUT_DIRECTORY "${TRT_OUT_DIR}"
 )
 
-set_target_properties(${STATIC_TARGET} PROPERTIES LINK_FLAGS "-Wl,--exclude-libs,ALL")
+#set_target_properties(${STATIC_TARGET} PROPERTIES LINK_FLAGS "-Wl,--exclude-libs,ALL")
 
 set_target_properties(${STATIC_TARGET} PROPERTIES DEBUG_POSTFIX ${TRT_DEBUG_POSTFIX})
 
diff --git a/plugin/InferPlugin.cpp b/plugin/InferPlugin.cpp
index b8e9121..8610e6f 100644
--- a/plugin/InferPlugin.cpp
+++ b/plugin/InferPlugin.cpp
@@ -26,6 +26,7 @@
 using namespace nvinfer1;
 using namespace nvinfer1::plugin;
 
+#include "axpyPlugin.h"
 #include "batchTilePlugin.h"
 #include "batchedNMSPlugin.h"
 #include "coordConvACPlugin.h"
@@ -182,6 +183,7 @@ bool initLibNvInferPlugins(void* logger, const char* libNamespace)
     initializePlugin<nvinfer1::plugin::ResizeNearestPluginCreator>(logger, libNamespace);
     initializePlugin<nvinfer1::plugin::RPROIPluginCreator>(logger, libNamespace);
     initializePlugin<nvinfer1::plugin::SpecialSlicePluginCreator>(logger, libNamespace);
+    initializePlugin<nvinfer1::plugin::AxpyPluginCreator>(logger, libNamespace);
     return true;
 }
 } // extern "C"
diff --git a/plugin/axpyPlugin/CMakeLists.txt b/plugin/axpyPlugin/CMakeLists.txt
new file mode 100644
index 0000000..77e2ec0
--- /dev/null
+++ b/plugin/axpyPlugin/CMakeLists.txt
@@ -0,0 +1,22 @@
+# DeepDetect
+# Copyright (c) 2019-2020 Jolibrain
+# Author:  Guillaume Infantes <guillaume.infantes@jolibrain.com>
+#
+# deepdetect is free software: you can redistribute it and/or modify
+# it under the terms of the GNU Lesser General Public License as published by
+# the Free Software Foundation, either version 3 of the License, or
+# (at your option) any later version.
+#
+# deepdetect is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+# GNU Lesser General Public License for more details.
+#
+# You should have received a copy of the GNU Lesser General Public License
+# along with deepdetect.  If not, see <http://www.gnu.org/licenses/>.
+file(GLOB SRCS *.cpp)
+set(PLUGIN_SOURCES ${PLUGIN_SOURCES} ${SRCS})
+set(PLUGIN_SOURCES ${PLUGIN_SOURCES} PARENT_SCOPE)
+file(GLOB CU_SRCS *.cu)
+set(BERT_CU_SOURCES ${BERT_CU_SOURCES} ${CU_SRCS})
+set(BERT_CU_SOURCES ${BERT_CU_SOURCES} PARENT_SCOPE)
diff --git a/plugin/axpyPlugin/axpyPlugin.cpp b/plugin/axpyPlugin/axpyPlugin.cpp
new file mode 100644
index 0000000..31e9603
--- /dev/null
+++ b/plugin/axpyPlugin/axpyPlugin.cpp
@@ -0,0 +1,120 @@
+#include "axpyPlugin.h"
+#include "iostream"
+
+using namespace nvinfer1;
+using nvinfer1::plugin::AxpyPlugin;
+using nvinfer1::plugin::AxpyPluginCreator;
+
+namespace
+{
+const char* AXPY_PLUGIN_VERSION{"1"};
+const char* AXPY_PLUGIN_NAME{"AXPY_TRT"};
+} // namespace
+
+PluginFieldCollection AxpyPluginCreator::mFC{};
+std::vector<PluginField> AxpyPluginCreator::mPluginAttributes;
+
+REGISTER_TENSORRT_PLUGIN(AxpyPluginCreator);
+
+Dims AxpyPlugin::getOutputDimensions(int index, const Dims* inputs, int nbInputDims)
+{
+    ASSERT(nbInputDims == 3);
+    ASSERT(index == 0);
+    std::cout << "i0 nb : " << inputs[0].nbDims << std::endl;
+    std::cout << "i0 d0 : " << inputs[0].d[0] << std::endl;
+    std::cout << "i0 d1 : " << inputs[0].d[1] << std::endl;
+    std::cout << "i0 d2 : " << inputs[0].d[2] << std::endl;
+    // dim is C x 1 x 1 ...
+    ASSERT(inputs[0].nbDims == 3);
+    std::cout << "i1 nb : " << inputs[1].nbDims << std::endl;
+    ASSERT(inputs[1].nbDims == 3);
+    std::cout << "i2 nb : " << inputs[2].nbDims << std::endl;
+    ASSERT(inputs[2].nbDims == 3);
+    std::cout << "i0 d0 : " << inputs[0].d[0] << std::endl;
+    std::cout << "i1 d0 : " << inputs[1].d[0] << std::endl;
+    std::cout << "i2 d0 : " << inputs[1].d[0] << std::endl;
+    ASSERT(inputs[0].d[0] == inputs[1].d[0]);
+    ASSERT(inputs[0].d[0] == inputs[2].d[0]);
+    std::cout << "i1 d1 : " << inputs[1].d[0] << std::endl;
+    ASSERT(inputs[1].d[1] == inputs[2].d[1]);
+    std::cout << "i1 d2 : " << inputs[1].d[0] << std::endl;
+    ASSERT(inputs[1].d[2] == inputs[2].d[2]);
+
+    return DimsCHW(inputs[0].d[0], inputs[1].d[1], inputs[1].d[2]);
+}
+
+int AxpyPlugin::enqueue(int batchSize, const void* const* inputs, void** outputs, void* workspace, cudaStream_t stream)
+{
+    float i0cpu[batchSize * _C];
+    float i1cpu[batchSize * _C * _H * _W];
+    float i2cpu[batchSize * _C * _H * _W];
+    //    float ocpu[batchSize * _C * _H * _W];
+    float* o = (float*) outputs[0];
+
+    // std::cout << "initted\n";
+
+    cudaMemcpy(i0cpu, inputs[0], batchSize * _C * sizeof(float), cudaMemcpyDeviceToHost);
+    cudaMemcpy(i1cpu, inputs[1], batchSize * _C * _H * _W * sizeof(float), cudaMemcpyDeviceToHost);
+    cudaMemcpy(i2cpu, inputs[2], batchSize * _C * _H * _W * sizeof(float), cudaMemcpyDeviceToHost);
+
+    for (int n = 0; n < batchSize; ++n)
+        for (int c = 0; c < _C; ++c)
+            for (int h = 0; h < _H; ++h)
+                for (int w = 0; w < _W; ++w)
+                {
+                    float res = i0cpu[n * _C + c] * i1cpu[n * _C * _H * _W + c * _H * _W + h * _W + w]
+                        + i2cpu[n * _C * _H * _W + c * _H * _W + h * _W + w];
+                    cudaMemcpy(
+                        &o[n * _C * _H * _W + c * _H * _W + h * _W + w], &res, sizeof(float), cudaMemcpyHostToDevice);
+                }
+    std::cerr << "computed\n";
+    return 0;
+}
+
+void AxpyPlugin::configurePlugin(const PluginTensorDesc* in, int nbInput, const PluginTensorDesc* out, int nbOutput)
+{
+    ASSERT(nbInput == 3);
+    ASSERT(nbOutput == 1);
+    ASSERT(in[0].type == DataType::kFLOAT && in[0].format == TensorFormat::kLINEAR);
+    ASSERT(in[1].type == DataType::kFLOAT && in[1].format == TensorFormat::kLINEAR);
+    ASSERT(in[2].type == DataType::kFLOAT && in[2].format == TensorFormat::kLINEAR);
+    ASSERT(out[0].type == DataType::kFLOAT && out[0].format == TensorFormat::kLINEAR);
+    // dim is C x 1 x 1 ...
+    ASSERT(in[0].dims.nbDims == 3);
+    ASSERT(in[1].dims.nbDims == 3);
+    ASSERT(in[2].dims.nbDims == 3);
+    ASSERT(out[0].dims.nbDims == 3);
+    ASSERT(in[0].dims.d[0] == in[1].dims.d[0]);
+    ASSERT(in[0].dims.d[0] == in[2].dims.d[0]);
+    ASSERT(in[1].dims.d[1] == in[2].dims.d[1]);
+    ASSERT(in[1].dims.d[2] == in[2].dims.d[2]);
+    _C = in[0].dims.d[0];
+    _H = in[1].dims.d[1];
+    _W = in[1].dims.d[2];
+    std::cout << "configured: " << _C << " " << _H << " " << _W << std::endl;
+}
+
+const char* AxpyPlugin::getPluginType() const
+{
+    return AXPY_PLUGIN_NAME;
+}
+
+const char* AxpyPlugin::getPluginVersion() const
+{
+    return AXPY_PLUGIN_VERSION;
+}
+
+const char* AxpyPluginCreator::getPluginName() const
+{
+    return AXPY_PLUGIN_NAME;
+}
+
+const char* AxpyPluginCreator::getPluginVersion() const
+{
+    return AXPY_PLUGIN_VERSION;
+}
+
+const PluginFieldCollection* AxpyPluginCreator::getFieldNames()
+{
+    return &mFC;
+}
diff --git a/plugin/axpyPlugin/axpyPlugin.cu b/plugin/axpyPlugin/axpyPlugin.cu
new file mode 100644
index 0000000..8f2d169
--- /dev/null
+++ b/plugin/axpyPlugin/axpyPlugin.cu
@@ -0,0 +1,63 @@
+/**
+ * DeepDetect
+ * Copyright (c) 2019-2020 Jolibrain
+ * Author:  Guillaume Infantes <guillaume.infantes@jolibrain.com>
+ *
+ * deepdetect is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU Lesser General Public License as published by
+ * the Free Software Foundation, either version 3 of the License, or
+ * (at your option) any later version.
+ *
+ * deepdetect is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public License
+ * along with deepdetect.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#include "plugin.h"
+#include <iostream>
+
+__global__ void axpyKernel(const int n, const int spatial_dim, const float* scale_data, const float* x_data,
+    const float* y_data, float* out_data)
+{
+    // this is exactly CUDA_KERNEL_LOOP(index, n)
+    for (int index = blockIdx.x * blockDim.x + threadIdx.x; index < n; index += gridDim.x * blockDim.x)
+    {
+        out_data[index] = // 1;
+            scale_data[index / spatial_dim] * x_data[index] + y_data[index];
+    }
+}
+
+namespace nvinfer1
+{
+namespace plugin
+{
+pluginStatus_t axpyInference(cudaStream_t stream, const int N, const int C, const int H, const int W, const void* ain,
+    const void* xin, const void* yin, void* outputData, void* workspace)
+{
+    const int dim = N * C * H * W;
+    const int spatial_vol = H * W;
+    float* a = (float*) const_cast<void*>(ain);
+    float* x = (float*) const_cast<void*>(xin);
+    float* y = (float*) const_cast<void*>(yin);
+    float* output = (float*) outputData;
+
+    constexpr int blockSize = 512;                    // CAFFE_CUDA_NUM_THREADS
+    int gridSize = (dim + blockSize - 1) / blockSize; // CAFFE_GET_BLOCKS(blockSize)
+
+    // below we also iterate over batch (N) directly, by setting dim to N x datadi (exactly like caffe impl)
+    // for (int n = 0; n < N; ++n)
+    // {
+    axpyKernel<<<gridSize, blockSize, 0, stream>>>(dim, spatial_vol, a, x, y, output);
+    //     a += C;
+    //     x += dim;
+    //     y += dim;
+    //     output += dim;
+    // }
+    return STATUS_SUCCESS;
+}
+} // namespace plugin
+} // namespace nvinfer1
diff --git a/plugin/axpyPlugin/axpyPlugin.h b/plugin/axpyPlugin/axpyPlugin.h
new file mode 100644
index 0000000..0da2770
--- /dev/null
+++ b/plugin/axpyPlugin/axpyPlugin.h
@@ -0,0 +1,210 @@
+/**
+ * DeepDetect
+ * Copyright (c) 2019-2020 Jolibrain
+ * Author:  Guillaume Infantes <guillaume.infantes@jolibrain.com>
+ *
+ * deepdetect is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU Lesser General Public License as published by
+ * the Free Software Foundation, either version 3 of the License, or
+ * (at your option) any later version.
+ *
+ * deepdetect is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public License
+ * along with deepdetect.  If not, see <http://www.gnu.org/licenses/>.
+ */
+#ifndef TRT_AXPY_PLUGIN_H
+#define TRT_AXPY_PLUGIN_H
+#include "cudnn.h"
+#include "plugin.h"
+#include <cublas_v2.h>
+#include <string>
+#include <vector>
+
+namespace nvinfer1
+{
+namespace plugin
+{
+// kernel header
+pluginStatus_t axpyInference(cudaStream_t stream, const int N, const int C, const int H, const int W, const void* ain,
+    const void* xin, const void* yin, void* outputData, void* workspace);
+
+class AxpyPlugin : public IPluginV2IOExt
+{
+public:
+    AxpyPlugin() {}
+
+    AxpyPlugin(int C, int H, int W)
+        : _C(C)
+        , _H(H)
+        , _W(W)
+    {
+    }
+
+    AxpyPlugin(const void* buffer, size_t length)
+    {
+        const char* d = static_cast<const char*>(buffer);
+        const char* a = d;
+        _C = read<int>(d);
+        _H = read<int>(d);
+        _W = read<int>(d);
+        ASSERT(d == a + length);
+    }
+
+    ~AxpyPlugin() override = default;
+
+    int getNbOutputs() const override
+    {
+        return 1;
+    }
+
+    Dims getOutputDimensions(int index, const Dims* inputs, int nbInputDims) override;
+
+    int initialize() override
+    {
+        return 0;
+    }
+
+    void terminate() override {}
+
+    size_t getWorkspaceSize(int maxBatchSize) const override
+    {
+        return (size_t) 0;
+    }
+
+    int enqueue(
+        int batchSize, const void* const* inputs, void** outputs, void* workspace, cudaStream_t stream) override;
+
+    size_t getSerializationSize() const override
+    {
+        return sizeof(int) * 3;
+    }
+
+    void serialize(void* buffer) const override
+    {
+        char *d = static_cast<char*>(buffer), *a = d;
+        write(d, _C);
+        write(d, _H);
+        write(d, _W);
+        ASSERT(d == a + getSerializationSize());
+    }
+
+    bool supportsFormatCombination(int pos, const PluginTensorDesc* inOut, int nbInputs, int nbOutputs) const override
+
+    {
+        return inOut[pos].format == TensorFormat::kLINEAR && (inOut[pos].type == DataType::kFLOAT);
+    }
+
+    const char* getPluginType() const override;
+
+    const char* getPluginVersion() const override;
+
+    void destroy() override
+    {
+        delete this;
+    }
+
+    IPluginV2Ext* clone() const override
+    {
+        // Create a new instance
+        IPluginV2Ext* plugin = new AxpyPlugin(_C, _H, _W);
+
+        // Set the namespace
+        plugin->setPluginNamespace(mNamespace.c_str());
+        return plugin;
+    }
+
+    void setPluginNamespace(const char* pluginNameSpace) override
+    {
+        mNamespace = pluginNameSpace;
+    }
+
+    const char* getPluginNamespace() const override
+    {
+        return mNamespace.c_str();
+    }
+
+    DataType getOutputDataType(int index, const nvinfer1::DataType* inputTypes, int nbInputs) const override
+    {
+        ASSERT(index == 0);
+        return DataType::kFLOAT;
+    }
+
+    bool isOutputBroadcastAcrossBatch(int outputIndex, const bool* inputIsBroadcasted, int nbInputs) const override
+    {
+        return false;
+    }
+
+    bool canBroadcastInputAcrossBatch(int inputIndex) const override
+    {
+        return false;
+    }
+
+    void attachToContext(cudnnContext* cudnnContext, cublasContext* cublasContext, IGpuAllocator* gpuAllocator) override
+    {
+        mCublas = cublasContext;
+    }
+
+    void configurePlugin(const PluginTensorDesc* in, int nbInput, const PluginTensorDesc* out, int nbOutput) override;
+
+    void detachFromContext() override {}
+
+private:
+    cublasHandle_t mCublas;
+
+    int _C{};
+    int _H{};
+    int _W{};
+    std::string mNamespace;
+};
+
+class AxpyPluginCreator : public IPluginCreator
+{
+public:
+    AxpyPluginCreator() {}
+
+    ~AxpyPluginCreator() override = default;
+
+    const char* getPluginName() const override;
+
+    const char* getPluginVersion() const override;
+
+    const PluginFieldCollection* getFieldNames() override;
+
+    IPluginV2Ext* deserializePlugin(const char* name, const void* serialData, size_t serialLength) override
+    {
+        AxpyPlugin* obj = new AxpyPlugin(serialData, serialLength);
+        obj->setPluginNamespace(mPluginNamespace.c_str());
+        return obj;
+    }
+
+    IPluginV2IOExt* createPlugin(const char* name, const PluginFieldCollection* fc) override
+    {
+        AxpyPlugin* obj = new AxpyPlugin();
+        obj->setPluginNamespace(mPluginNamespace.c_str());
+        return obj;
+    }
+
+    void setPluginNamespace(const char* pluginNamespace) override
+    {
+        mPluginNamespace = pluginNamespace;
+    }
+
+    const char* getPluginNamespace() const override
+    {
+        return mPluginNamespace.c_str();
+    }
+
+private:
+    static PluginFieldCollection mFC;
+    static std::vector<PluginField> mPluginAttributes;
+    std::string mPluginNamespace;
+};
+
+} // namespace plugin
+} // namespace nvinfer1
+
+#endif
